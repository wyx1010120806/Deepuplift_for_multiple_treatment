{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4540cca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c1499a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from evaluate import *\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "from torch.nn import *\n",
    "from urllib.parse import urlparse, urlunparse\n",
    "from pathlib import Path\n",
    "from uplifttree import *\n",
    "from functools import partial\n",
    "from esn_tarnet import *\n",
    "from feature_select import *\n",
    "from s_learner import *\n",
    "from t_learner import *\n",
    "from tarnet import *\n",
    "from dragonnet import *\n",
    "from x_learner import *\n",
    "from descn import *\n",
    "from uplifttree import *\n",
    "from cfrnet import *\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "def set_seed(seed):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaefa01",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_shell(hdfs_ls_cmd):\n",
    "    print(hdfs_ls_cmd)\n",
    "    result = subprocess.run(hdfs_ls_cmd, shell=True, capture_output=True, text=True)     \n",
    "    if result.returncode != 0:\n",
    "        print(f\"Error listing files in HDFS: {result.stderr}\")\n",
    "    print(result)\n",
    "    print(result.returncode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39c0695",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 读取JSON文件并转换为字典\n",
    "with open('/opt/tiger/rh2_params/custom_template_vars.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "\n",
    "# 打印字典\n",
    "print(data)\n",
    "if \"selected_features_path\" in data.keys():\n",
    "    feature_path = data['selected_features_path']\n",
    "else:\n",
    "    feature_path = data['feature_path']\n",
    "    \n",
    "feature = Path(feature_path).name\n",
    "p = urlparse(feature_path)\n",
    "parent_path = p.path.rsplit('/', 1)[0]  \n",
    "selected_path = urlunparse((p.scheme, p.netloc, parent_path, '', '', ''))\n",
    "\n",
    "feature_list_discrete_path = os.path.basename(feature_path).split('.')[0] + '_discrete.pkl'\n",
    "feature_list_discrete_hdfs_path = selected_path + '/' + feature_list_discrete_path\n",
    "discrete_size_cols_path = os.path.basename(feature_path).split('.')[0] + '_discrete_size.pkl'\n",
    "discrete_size_cols_hdfs_path = selected_path + '/' + discrete_size_cols_path\n",
    "\n",
    "train_data_path = data['train_data_path']\n",
    "train_data_path_name = Path(train_data_path).name\n",
    "\n",
    "test_data_path = data['test_data_path']\n",
    "test_data_path_name = Path(test_data_path).name\n",
    "\n",
    "s = data.get('treatment_label_list', '')\n",
    "treatment_label_list = [int(x.strip()) for x in s.split(',') if x.strip()]\n",
    "\n",
    "y = data['y']\n",
    "treatment_col = data['treatment_col']\n",
    "s = data.get('treatment_label_list', '')\n",
    "treatment_label_list = [int(x.strip()) for x in s.split(',') if x.strip()]\n",
    "\n",
    "task = data['task']\n",
    "model_path_list = data['model_save_path']\n",
    "model_params_list = data['model_params']\n",
    "model_type_list = data['model_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2195c4fc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "run_shell(f\"rm -r ./{test_data_path_name}\")\n",
    "run_shell(f\"hdfs dfs -get {test_data_path} ./\")\n",
    "\n",
    "# run_shell(f\"rm -r ./{train_data_path_name}\")\n",
    "# run_shell(f\"hdfs dfs -get {train_data_path} ./\")\n",
    "\n",
    "run_shell(f\"rm -r ./{feature}\")\n",
    "run_shell(f\"hdfs dfs -get {feature_path} ./\")\n",
    "\n",
    "run_shell(f\"rm -r ./{feature_list_discrete_path}\")\n",
    "run_shell(f\"hdfs dfs -get {feature_list_discrete_hdfs_path} ./\")\n",
    "\n",
    "run_shell(f\"rm -r ./{discrete_size_cols_path}\")\n",
    "run_shell(f\"hdfs dfs -get {discrete_size_cols_hdfs_path} ./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711656bf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#读取特征列表\n",
    "with open(f\"./{feature}\", 'rb') as f:\n",
    "    feature_list = pickle.load(f)\n",
    "print(len(feature_list))\n",
    "\n",
    "with open(f\"./{feature_list_discrete_path}\", 'rb') as f:\n",
    "    feature_list_discrete = pickle.load(f)\n",
    "print(len(feature_list_discrete))\n",
    "\n",
    "with open(f\"./{discrete_size_cols_path}\", 'rb') as f:\n",
    "    discrete_size_cols = pickle.load(f)\n",
    "print(len(discrete_size_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f627648",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "model_list = {}\n",
    "for i in range(len(model_path_list)):\n",
    "    model_type = model_type_list[i]\n",
    "    model_path = model_path_list[i]\n",
    "    model_params = model_params_list[i]\n",
    "\n",
    "    if not model_params:\n",
    "        if model_type == 'tarnet':\n",
    "            model_params = dict(\n",
    "                embedding_dim=3,share_dim=64,\n",
    "                share_hidden_dims =[256,128,128,64],\n",
    "                base_hidden_dims = [64,32,32,16],output_activation_base=None,\n",
    "                share_hidden_func = torch.nn.ELU(),base_hidden_func = torch.nn.ELU()\n",
    "            )\n",
    "        elif model_type == 'esn_tarnet':\n",
    "            model_params = dict(\n",
    "                embedding_dim=3,share_dim=64,\n",
    "                share_hidden_dims =[256,128,64,64],\n",
    "                base_hidden_dims=[64,32,32,16],output_activation_base=torch.nn.Sigmoid(),\n",
    "                ipw_hidden_dims=[256,128,64,64],output_activation_ipw=None,\n",
    "                share_hidden_func = torch.nn.ELU(),base_hidden_func = torch.nn.ELU(), ipw_hidden_func = torch.nn.ELU()\n",
    "            )\n",
    "        elif model_type == 'slearner':\n",
    "            model_params = dict(\n",
    "                embedding_dim=3,\n",
    "                base_hidden_dims=[64,32,32,16],output_activation_base=None,base_hidden_func = torch.nn.ELU()\n",
    "            )\n",
    "        elif model_type == 'tlearner':\n",
    "            model_params = dict(\n",
    "                embedding_dim=3,\n",
    "                base_hidden_dims=[64,32,32,16],output_activation_base=None,base_hidden_func = torch.nn.ELU()\n",
    "            )\n",
    "        elif model_type == 'xlearner':\n",
    "            model_params = dict(\n",
    "                embedding_dim=3,\n",
    "                base_hidden_dims=[64,32,32,16],output_activation_base=torch.nn.Sigmoid(),base_hidden_func = torch.nn.ELU()\n",
    "                ,lift_hidden_dims=[64,32,32,16],lift_activation_base=None,lift_hidden_func = torch.nn.ELU()\n",
    "                ,treatment_label_weight=[0.25,0.25,0.25,0.25],lift_weight=[0.5,0.5,0.5,0.5]\n",
    "            )\n",
    "        elif model_type == 'descn':\n",
    "            model_params = dict(\n",
    "                embedding_dim=3,share_dim=64,\n",
    "                        share_hidden_dims =[256,128,64,64],\n",
    "                        base_hidden_dims=[64,32,32,16],output_activation_base=None,\n",
    "                        ipw_hidden_dims=[64,32,32,16],output_activation_ipw=None,\n",
    "                        pseudo_hidden_dims=[64,32,32,16],output_activation_pseudo=None,\n",
    "                        share_hidden_func = torch.nn.ELU(),base_hidden_func = torch.nn.ELU(), ipw_hidden_func = torch.nn.ELU(),pseudo_hidden_func = torch.nn.ELU()\n",
    "            )\n",
    "        elif model_type == 'dragonnet':\n",
    "            model_params = dict(\n",
    "                embedding_dim=3,share_dim=64,\n",
    "                share_hidden_dims =[512,256,256,128], share_hidden_func = torch.nn.ELU(),\n",
    "                base_hidden_dims=[64,32,32,16],output_activation_base=torch.nn.Sigmoid(),base_hidden_func = torch.nn.ELU(),\n",
    "                ipw_hidden_dims=[64,32,32,16],output_activation_ipw=torch.nn.Sigmoid(),ipw_hidden_func = torch.nn.ELU(),\n",
    "                epsilons_hidden_dims=[64,32,32,16],output_activation_epsilons=torch.nn.Sigmoid(),epsilons_hidden_func = torch.nn.ELU()\n",
    "            )\n",
    "        elif model_type == 'cfrnet':\n",
    "            model_params = dict(\n",
    "                embedding_dim=3,share_dim=64,\n",
    "                share_hidden_dims =[256,128,128,64],\n",
    "                base_hidden_dims = [64,32,32,16],output_activation_base=None,\n",
    "                share_hidden_func = torch.nn.ELU(),base_hidden_func = torch.nn.ELU()\n",
    "            )\n",
    "        elif model_type == 'uplifttree':\n",
    "            model_params = dict(\n",
    "                max_depth=5\n",
    "            )\n",
    "        elif model_type == 'upliftforest':\n",
    "            model_params = dict(\n",
    "                max_depth=5,n_estimators=100\n",
    "            )\n",
    "        elif model_type == 'causalforestdml':\n",
    "            model_params = dict(\n",
    "                max_depth=5,n_estimators=100\n",
    "            )   \n",
    "        else:\n",
    "            raise ValueError(\"model_type must be 'tarnet', 'cfrnet', 'esn_tarnet', 'slearner', 'tlearner', 'xlearner', 'descn', 'dragonnet', 'uplifttree','upliftforest','causalforestdml'\")\n",
    "\n",
    "    if model_type == 'tarnet':\n",
    "        model = Tarnet(\n",
    "            input_dim=len(feature_list), discrete_size_cols=discrete_size_cols,\n",
    "            treatment_label_list=treatment_label_list,device=device,model_type = model_type,task=task,classi_nums=2,\n",
    "            **model_params\n",
    "        ).to(device)\n",
    "        loss_f = partial(tarnet_loss)\n",
    "    elif model_type == 'esn_tarnet':\n",
    "        model = ESN_Tarnet(\n",
    "            input_dim=len(feature_list), discrete_size_cols=discrete_size_cols,\n",
    "            treatment_label_list=treatment_label_list,device=device,model_type = model_type,task=task,classi_nums=2,\n",
    "            **model_params\n",
    "        ).to(device)\n",
    "        loss_f = partial(esn_tarnet_loss)\n",
    "    elif model_type == 'slearner':\n",
    "        model = Slearner(\n",
    "            input_dim=len(feature_list), discrete_size_cols=discrete_size_cols,\n",
    "            treatment_label_list=treatment_label_list,device=device,model_type = model_type,task=task,classi_nums=2,\n",
    "            **model_params\n",
    "        ).to(device)\n",
    "        loss_f = partial(slearn_loss)\n",
    "    elif model_type == 'tlearner':\n",
    "        model = Tlearner(\n",
    "            input_dim=len(feature_list), discrete_size_cols=discrete_size_cols,\n",
    "            treatment_label_list=treatment_label_list,device=device,model_type = model_type,task=task,classi_nums=2,\n",
    "            **model_params\n",
    "        ).to(device)\n",
    "        loss_f = partial(tlearn_loss)\n",
    "    elif model_type == 'xlearner':\n",
    "        model = Xlearner(\n",
    "            input_dim=len(feature_list), discrete_size_cols=discrete_size_cols,\n",
    "            treatment_label_list=treatment_label_list,device=device,model_type = model_type,task=task,classi_nums=2,\n",
    "            **model_params\n",
    "        ).to(device)\n",
    "        loss_f = partial(xlearn_loss)\n",
    "    elif model_type == 'descn':\n",
    "        model = Descn(\n",
    "            input_dim=len(feature_list), discrete_size_cols=discrete_size_cols,\n",
    "            treatment_label_list=treatment_label_list,device=device,model_type = model_type,task=task,classi_nums=2,\n",
    "            **model_params\n",
    "        ).to(device)\n",
    "        loss_f = partial(descn_loss)\n",
    "    elif model_type == 'dragonnet':\n",
    "        model = Dragonnet(\n",
    "            input_dim=len(feature_list), discrete_size_cols=discrete_size_cols,\n",
    "            treatment_label_list=treatment_label_list,device=device,model_type = model_type,task=task,classi_nums=2,\n",
    "            **model_params\n",
    "        ).to(device)\n",
    "        loss_f = partial(dragonnet_loss)\n",
    "    elif model_type == 'cfrnet':\n",
    "        model = Cfrnet(\n",
    "            input_dim=len(feature_list), discrete_size_cols=discrete_size_cols,\n",
    "            treatment_label_list=treatment_label_list,device=device,model_type = model_type,task=task,classi_nums=2,\n",
    "            **model_params\n",
    "        ).to(device)\n",
    "        loss_f = partial(cfrnet_loss)\n",
    "    elif model_type == 'uplifttree':\n",
    "        model = UpliftTreeModel(task=task,model_type='tree',treatment_list=treatment_label_list,features_list=feature_list,**model_params)\n",
    "    elif model_type == 'upliftforest':\n",
    "        model = UpliftTreeModel(task=task,model_type='forest',treatment_list=treatment_label_list,features_list=feature_list,**model_params)\n",
    "    elif model_type == 'causalforestdml':\n",
    "        model = UpliftTreeModel(task=task,model_type='causalforestdml',treatment_list=treatment_label_list,features_list=feature_list,**model_params)\n",
    "    else:\n",
    "        raise ValueError(\"model_type must be 'tarnet', 'cfrnet', 'esn_tarnet', 'slearner', 'tlearner', 'xlearner', 'descn', 'dragonnet', 'uplifttree','upliftforest','causalforestdml'\")\n",
    "    \n",
    "    model_path_name = Path(model_path).name\n",
    "    run_shell(f\"rm -r ./{model_path_name}\")\n",
    "    run_shell(f\"hdfs dfs -get {model_path} ./\")\n",
    "\n",
    "    if model_type in ['uplifttree','upliftforest','causalforestdml']:\n",
    "        import pickle\n",
    "        with open(f\"./{model_path_name}\", 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "            model_list[model_type] = model\n",
    "    else:\n",
    "        checkpoint = torch.load(f\"./{model_path_name}\", map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model_list[model_type] = model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342c36c6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# train_df = pd.read_parquet(f\"./{train_data_path_name}\",columns=feature_list+[treatment_col,y]).fillna(0)\n",
    "test_df = pd.read_parquet(f\"./{test_data_path_name}\",columns=feature_list+[treatment_col,y]).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80913589",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "X = test_df[feature_list].values\n",
    "X_discrete_test = torch.tensor(test_df[feature_list_discrete].values, dtype=torch.float32).to(device)\n",
    "X_continuous_test = torch.tensor(test_df[[_ for _ in feature_list if _ not in feature_list_discrete]].values, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f149bd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "uplift_prediction = {}\n",
    "for each in model_list.keys():\n",
    "    if each in ['uplifttree','upliftforest','causalforestdml']:\n",
    "        model = model_list[each]\n",
    "        uplift_prediction[each] = model.predict(X).values\n",
    "    else:\n",
    "        model = model_list[each]\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        with torch.no_grad(): \n",
    "            uplift_predictions,y_preds,*eps = model(None, None, X_discrete=X_discrete_test, X_continuous=X_continuous_test)\n",
    "        uplift_prediction[each] = uplift_predictions.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8895d6a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "for treatment in treatment_label_list[1:]:\n",
    "    print(f'*********************** treatment标识:{treatment} ***********************')\n",
    "    for each in uplift_prediction.keys():\n",
    "        test_df[each] = uplift_prediction[each][:,treatment-1]\n",
    " \n",
    "    df_tmp = test_df[test_df[treatment_col].isin([0,treatment])]\n",
    "    df_tmp[treatment_col] = df_tmp[treatment_col].apply(lambda x: 1 if x == treatment else 0)\n",
    "    df_tmp['random'] = np.random.rand(df_tmp.shape[0])\n",
    "\n",
    "    evaluate(df=df_tmp[[y,treatment_col,'random'] + [_ for _ in uplift_prediction.keys()]],y_true=y,treatment=treatment_col,divide_feature=None,n=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f4d264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = train_df.sample(n=100)\n",
    "# df_test = test_df.sample(n=100)\n",
    "\n",
    "# for treatment in treatment_label_list[1:]:\n",
    "#     print(f'*********************** treatment标识:{treatment} ***********************')\n",
    "#     for each in model_list.keys():\n",
    "#         if each not in ['uplifttree','upliftforest','causalforestdml']:\n",
    "#             print(f'*********************** 模型:{each} 特征shap图如下 ***********************')\n",
    "#             model = model_list[each]\n",
    "#             feature_importance_with_shap(model, df_train,df_test,feature_list,feature_list_discrete,device,treatment)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
